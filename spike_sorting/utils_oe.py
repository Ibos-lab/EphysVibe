# Tools for pre-processing OpenEphis data
import os
import glob
import h5py
import logging
from open_ephys.analysis import Session
import numpy as np
import pandas as pd
import re
from scipy.signal import butter, lfilter
import data_structure, config


def load_oe_data(directory):
    """Load OpenEphis data

    Args:
        directory (Path): path to the directory

    Returns:
        session (Session): session object
        subject (str): name of the subject
        date_time (str): date and time of the recording
        areas (list): name of the recorded areas
    """
    logging.info("Loading OE data")
    # directory = bhv_path.parents[3]
    session = Session(directory)
    split_dir = os.path.normpath(directory).split(os.sep)
    date_time = split_dir[-1]
    subject = split_dir[-2]
    # check nodes
    areas = []
    for node in session.recordnodes:
        node_path = node.directory
        n_area = re.split(r"[ ]", node_path.split(os.sep)[-1])[-1]
        areas.append(n_area)

    return session, subject, date_time, areas


def load_dat_file(dat_path, shape_0, shape_1):
    dat_file = np.memmap(dat_path, mode="r", dtype="int16", shape=(shape_0, shape_1)).T
    return dat_file


def load_event_files(event_path):
    timestamp = np.load(glob.glob("/".join([event_path] + ["timestamps.npy"]))[0])
    channel = np.load(glob.glob("/".join([event_path] + ["channels.npy"]))[0])
    state = np.load(glob.glob("/".join([event_path] + ["channel_states.npy"]))[0])
    state = np.where(state > 0, 1, 0)
    events = {"timestamp": timestamp, "channel": channel, "state": state}
    return events


def load_bhv_data(directory, subject):
    """Load continuous and behavioral data.

    Args:
        bhv_path (Path): path to the bhv file generated by MonkeyLogic
        spike_path (Path): path to the Kilosort folder containing the sorted signals
        config_file (Path): path to the .json file containing the recorded areas

    Returns:
        bhv (group): bhv file
    """
    # Load behavioral data
    bhv_path = os.path.normpath(str(directory) + "/*" + subject + ".h5")
    bhv_path = glob.glob(bhv_path, recursive=True)
    if len(bhv_path) == 0:
        logging.info("Bhv file not found")
    logging.info("Loading bhv data")
    bhv = h5py.File(bhv_path[0], "r")["ML"]

    return bhv


def load_eyes(s_path, shape_0, shape_1, start_time=0):

    # load eyes data
    eyes_path = "/".join(s_path[:-1] + ["Record Node eyes"] + ["eyes.dat"])
    continuous = load_dat_file(eyes_path, shape_0=shape_0, shape_1=shape_1)
    # downsample signal
    eyes_ds = signal_downsample(
        continuous[:, start_time:], config.DOWNSAMPLE, idx_start=0, axis=1
    )

    return eyes_ds


def load_spike_data(spike_path):
    """Load spikes data

    Args:
        spike_path (str): path to the kilosort folder

    Returns:
        idx_spiketimes (np.array): array containing the spike times
        spiketimes_clusters_id (memmap): array containing to which neuron the spike times belongs to
        cluster_info (pd.Dataframe): info about the clusters
    """
    # search kilosort folder
    logging.info("Loading spikes data")
    idx_spiketimes = np.load(spike_path + "/spike_times.npy", "r").reshape(-1) - 1
    spiketimes_clusters_id = np.load(spike_path + "/spike_clusters.npy", "r")  #
    cluster_info = pd.read_csv(
        spike_path + "/cluster_info.tsv", sep="\t"
    )  # info of each cluster
    # ignore noisy groups
    cluster_info = cluster_info[cluster_info["group"] != "noise"]
    return idx_spiketimes, spiketimes_clusters_id, cluster_info


def signal_downsample(x, downsample, idx_start=0, axis=1):
    idx_ds = np.arange(idx_start, x.shape[axis], downsample)
    if axis == 1:
        return x[:, idx_ds]
    return x[idx_ds]


def butter_lowpass_filter(data, fc, fs, order=5, downsample=30):

    b, a = butter(N=order, Wn=fc, fs=fs, btype="low", analog=False)
    y = np.zeros((data.shape[0], int(np.floor(data.shape[1] / downsample)) + 1))

    for i_data in range(data.shape[0]):
        y_f = lfilter(b, a, data[i_data])
        y[i_data] = signal_downsample(y_f, downsample, idx_start=0, axis=0)
    return y


def select_timestamps(c_timestamps, e_timestamps, fs, t_before_event=10, downsample=30):
    # Select the timestamps of continuous data from t sec before the first event occurs
    # This is done to reduce the data
    start_time = np.where(c_timestamps == e_timestamps[0])[0]
    start_time = (
        start_time[0] if start_time.shape[0] > 0 else 0
    )  # check if not empty, else we select all data
    start_time = (
        start_time - fs * t_before_event if start_time - fs * t_before_event > 0 else 0
    )  # check if start_time - fs*t >0, else we select all data
    # select timestamps from start_time and donwsample
    ds_timestamps = signal_downsample(
        c_timestamps, downsample, idx_start=start_time, axis=0
    )

    return ds_timestamps, start_time


def reconstruct_8bits_words(real_strobes, e_channel, e_state):
    idx_old = 0
    current_8code = np.zeros(8, dtype=np.int64)
    full_word = np.zeros(len(real_strobes))

    for n_strobe, idx_strobe in enumerate(real_strobes):

        for ch in np.arange(0, 7):

            idx_ch = np.where(e_channel[idx_old:idx_strobe] == ch + 1)[0]

            current_8code[7 - ch] = (
                e_state[idx_ch[-1]] if idx_ch.size != 0 else current_8code[7 - ch]
            )

        full_word[n_strobe] = int("".join([str(item) for item in current_8code]), 2)

    return full_word


def check_strobes(bhv, full_word, real_strobes):
    # Check if strobe and codes number match
    bhv_codes = []
    trials = list(bhv.keys())[1:-1]
    for i_trial in trials:
        bhv_codes.append(list(bhv[i_trial]["BehavioralCodes"]["CodeNumbers"])[0])
    bhv_codes = np.concatenate(bhv_codes)

    if full_word.shape[0] != real_strobes.shape[0]:
        logging.info("Warning, Strobe and codes number do not match")
        logging.info("Strobes =", real_strobes.shape[0])
        logging.info("codes number =", full_word.shape[0])
    else:
        logging.info("Strobe and codes number do match")
        logging.info("Strobes = %d", real_strobes.shape[0])
        logging.info("codes number = %d", full_word.shape[0])

    if full_word.shape[0] != bhv_codes.shape[0]:
        logging.info("Warning, ML and OE code numbers do not match")
    else:
        logging.info("ML and OE code numbers do match")
        if np.sum(bhv_codes - full_word) != 0:
            logging.info("Warning, ML and OE codes are different")
        else:
            logging.info("ML and OE codes are the same")


def find_events_codes(events, bhv):

    # Reconstruct 8 bit words
    logging.info("Reconstructing 8 bit words")
    idx_real_strobes = np.where(
        np.logical_and(
            np.logical_and(events["channel"] == 8, events["state"] == 1),
            events["timestamp"] > 0,
        )
    )[
        0
    ]  # state 1: ON, state 0: OFF
    full_word = reconstruct_8bits_words(
        idx_real_strobes, e_channel=events["channel"], e_state=events["state"]
    )
    # Check if strobe and codes number match
    check_strobes(bhv, full_word, idx_real_strobes)

    real_strobes = events["timestamp"][idx_real_strobes]
    start_trials = real_strobes[
        full_word == config.START_CODE
    ]  # timestamps where trials starts

    # search number of blocks
    trial_keys = list(bhv.keys())[1:-1]
    n_trials = len(trial_keys)
    blocks = []
    # iterate over trials
    for trial_i in range(n_trials):
        blocks.append(bhv[trial_keys[trial_i]]["Block"][0][0])

    # change bhv structure
    bhv = np.array(data_structure.bhv_to_dictionary(bhv))

    return (full_word, real_strobes, start_trials, blocks, bhv)


def compute_lfp(c_samples):
    # Compute LFP

    LFP_ds = butter_lowpass_filter(
        c_samples,
        fc=config.FC,
        fs=config.FS,
        order=config.ORDER,
        downsample=config.DOWNSAMPLE,
    )

    return LFP_ds
