# Tools for pre-processing OpenEphis data
import os
import glob
import h5py
import logging
from open_ephys.analysis import Session
import numpy as np
import pandas as pd
import re
from scipy.signal import butter, lfilter
import data_structure, config


def load_oe_data(directory):
    """Load OpenEphis data

    Args:
        directory (Path): path to the directory

    Returns:
        session (Session): session object
        subject (str): name of the subject
        date_time (str): date and time of the recording
        areas (list): name of the recorded areas
    """
    logging.info("Loading OE data")
    # directory = bhv_path.parents[3]
    session = Session(directory)
    split_dir = os.path.normpath(directory).split(os.sep)
    date_time = split_dir[-1]
    subject = split_dir[-2]
    # check nodes
    areas = []
    for node in session.recordnodes:
        node_path = node.directory
        n_area = re.split(r"[ ]", node_path.split(os.sep)[-1])[-1]
        areas.append(n_area)

    return session, subject, date_time, areas


def load_bhv_data(directory, subject):
    """Load continuous and behavioral data.

    Args:
        bhv_path (Path): path to the bhv file generated by MonkeyLogic
        spike_path (Path): path to the Kilosort folder containing the sorted signals
        config_file (Path): path to the .json file containing the recorded areas

    Returns:
        bhv (group): bhv file
    """
    # Load behavioral data
    bhv_path = os.path.normpath(str(directory) + "/*" + subject + ".h5")
    bhv_path = glob.glob(bhv_path, recursive=True)
    if len(bhv_path) == 0:
        logging.info("Bhv file not found")
    bhv_path = bhv_path[0]
    logging.info("Loading bhv data")
    bhv = h5py.File(bhv_path, "r")["ML"]

    return bhv


def load_spike_data(spike_path):
    """Load spikes data

    Args:
        spike_path (str): path to the kilosort folder

    Returns:
        idx_spiketimes (np.array): array containing the spike times
        spiketimes_clusters_id (memmap): array containing to which neuron the spike times belongs to
        cluster_info (pd.Dataframe): info about the clusters
    """
    # search kilosort folder
    logging.info("Loading spikes data")
    idx_spiketimes = np.load(spike_path + "/spike_times.npy", "r").reshape(-1) - 1
    spiketimes_clusters_id = np.load(spike_path + "/spike_clusters.npy", "r")  #
    cluster_info = pd.read_csv(
        spike_path + "/cluster_info.tsv", sep="\t"
    )  # info of each cluster
    return idx_spiketimes, spiketimes_clusters_id, cluster_info


def signal_downsample(x, downsample, idx_start=0, axis=0):

    idx_ds = np.arange(idx_start, x.shape[axis], downsample)
    if axis == 0:
        return x[idx_ds]

    return x[:, idx_ds]


def butter_lowpass_filter(data, fc, fs, order=5, downsample=30):

    b, a = butter(N=order, Wn=fc, fs=fs, btype="low", analog=False)
    y = np.zeros((data.shape[1], int(np.floor(data.shape[0] / downsample)) + 1))

    for i_data in range(data.shape[1]):
        y_f = lfilter(b, a, data[:, i_data])
        y[i_data] = signal_downsample(y_f, downsample, idx_start=0, axis=0)
    return y


def select_timestamps(
    c_timestamps, e_timestamps, idx_spiketimes, fs, t_before_event=10, downsample=30
):
    # Select the timestamps of continuous data from t sec before the first event occurs
    # This is done to reduce the data
    start_time = np.where(c_timestamps == e_timestamps[0])[0]
    start_time = (
        start_time[0] if start_time.shape[0] > 0 else 0
    )  # check if not empty, else we select all data
    start_time = (
        start_time - fs * t_before_event if start_time - fs * t_before_event > 0 else 0
    )  # check if start_time - fs*t >0, else we select all data
    # select timestamps from start_time and donwsample
    filtered_timestamps = signal_downsample(c_timestamps, downsample, start_time)

    spiketimes = c_timestamps[idx_spiketimes]  # timestamps of all the spikes

    return filtered_timestamps, start_time, spiketimes


def reconstruct_8bits_words(real_strobes, e_channel, e_state):
    idx_old = 0
    current_8code = np.zeros(8, dtype=np.int64)
    full_word = np.zeros(len(real_strobes))

    for n_strobe, idx_strobe in enumerate(real_strobes):

        for ch in np.arange(0, 7):

            idx_ch = np.where(e_channel[idx_old:idx_strobe] == ch + 1)[0]

            current_8code[7 - ch] = (
                e_state[idx_ch[-1]] if idx_ch.size != 0 else current_8code[7 - ch]
            )

        full_word[n_strobe] = int("".join([str(item) for item in current_8code]), 2)

    return full_word


def check_strobes(bhv, full_word, real_strobes):
    # Check if strobe and codes number match
    bhv_codes = []
    trials = list(bhv.keys())[1:-1]
    for i_trial in trials:
        bhv_codes.append(list(bhv[i_trial]["BehavioralCodes"]["CodeNumbers"])[0])
    bhv_codes = np.concatenate(bhv_codes)

    if full_word.shape[0] != real_strobes.shape[0]:
        logging.info("Warning, Strobe and codes number do not match")
        logging.info("Strobes =", real_strobes.shape[0])
        logging.info("codes number =", full_word.shape[0])
    else:
        logging.info("Strobe and codes number do match")
        logging.info("Strobes = %d", real_strobes.shape[0])
        logging.info("codes number = %d", full_word.shape[0])

    if full_word.shape[0] != bhv_codes.shape[0]:
        logging.info("Warning, ML and OE code numbers do not match")
    else:
        logging.info("ML and OE code numbers do match")
        if np.sum(bhv_codes - full_word) != 0:
            logging.info("Warning, ML and OE codes are different")
        else:
            logging.info("ML and OE codes are the same")


def find_events_codes(events, bhv):

    # Reconstruct 8 bit words
    logging.info("Reconstructing 8 bit words")
    idx_real_strobes = np.where(
        np.logical_and(
            np.logical_and(events.channel == 8, events.state == 1), events.timestamp > 0
        )
    )[
        0
    ]  # state 1: ON, state 0: OFF
    full_word = reconstruct_8bits_words(
        idx_real_strobes, e_channel=events.channel, e_state=events.state
    )
    # Check if strobe and codes number match
    check_strobes(bhv, full_word, idx_real_strobes)

    real_strobes = events.timestamp[idx_real_strobes].values
    start_trials = real_strobes[
        full_word == config.START_CODE
    ]  # timestamps where trials starts

    # search number of blocks
    trial_keys = list(bhv.keys())[1:-1]
    n_trials = len(trial_keys)
    blocks = []
    # iterate over trials
    for trial_i in range(n_trials):
        blocks.append(bhv[trial_keys[trial_i]]["Block"][0][0])

    # change bhv structure
    bhv = np.array(data_structure.bhv_to_dictionary(bhv))

    bl_start_trials = []
    bhv_trials = []

    for block in np.unique(blocks):
        idx_block = np.where(blocks == block)[0]

        bl_start_trials.append(start_trials[idx_block])
        bhv_trials.append(bhv[idx_block])

    return (
        full_word,
        real_strobes,
        bl_start_trials,
        np.unique(blocks),
        bhv_trials,
    )


def compute_lfp(c_samples, start_time):
    # Compute LFP

    LFP_ds = butter_lowpass_filter(
        c_samples[start_time:, :-2],
        fc=config.FC,
        fs=config.FS,
        order=config.ORDER,
        downsample=config.DOWNSAMPLE,
    )
    eyes_ds = signal_downsample(
        c_samples[start_time:, -2:].reshape(2, -1),
        config.DOWNSAMPLE,
        idx_start=0,
        axis=1,
    )
    return LFP_ds, eyes_ds
